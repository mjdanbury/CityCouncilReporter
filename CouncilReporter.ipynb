{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import dotenv\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "retriever = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_summary(my_url):\n",
    "\n",
    "    loader = YoutubeLoader.from_youtube_url(\n",
    "        my_url, add_video_info=True\n",
    "    )\n",
    "\n",
    "    global docs\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    vec_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "    )\n",
    "\n",
    "    vec_splits = vec_splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=vec_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "    global retriever\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={\"k\": 6})\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "    map_template = \"\"\"The following is a transcript from a New York City Council Meeting.\n",
    "    {docs}\n",
    "    Based on this transcript, please identify angles that a journalist covering this city council meeting might want to write a story about. These do not have to be fully fleshed out stories. Rather, they should be leads that the journalist would follow up on with rigorous reporting. Please include the most relevant quote from the transcript for each angle.\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "    reduce_template = \"\"\"The following is set of angles that a journalist might wish to pursue in their reporiting:\n",
    "    {docs}\n",
    "    Take these and distill it into a final, consolidated list of angles to follow up on. \n",
    "    Helpful Answer:\"\"\"\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    "    )\n",
    "\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4000,\n",
    "    )\n",
    "\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=False,\n",
    "    )\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=10000, chunk_overlap=2000, add_start_index=True\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    return map_reduce_chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_respond(message, history):\n",
    "\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "    condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "    condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", condense_q_system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    condense_q_chain = condense_q_prompt | llm | StrOutputParser()\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Incorporate any important context but keep your answer concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def condense_question(input: dict):\n",
    "        if input.get(\"chat_history\"):\n",
    "            return condense_q_chain\n",
    "        else:\n",
    "            return input[\"question\"]\n",
    "        \n",
    "    rag_chain = (\n",
    "        RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "    )\n",
    "    \n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    gpt_response = rag_chain.invoke({\"question\":message,\"chat_history\":history_langchain_format})\n",
    "    return gpt_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# New York City Council AI Reporter\")\n",
    "    gr.Markdown(\"Find [a video](https://www.youtube.com/@NYCCouncil/streams) of a past New York City Council meeting and paste the URL below, or for a quick demo, use the one provided. The AI reporter will analyze the video transcript and suggest story angles for you to pursue. This will take some timeâ€”roughly one minute for every 15 minutes of transcript (90 seconds for the demo). When it finishes, you may ask it any followup questions and it will incorporate any relevant information it gleaned in its answers.\")\n",
    "    with gr.Row():\n",
    "        url_input = gr.Textbox(value=\"https://www.youtube.com/watch?v=FzCLB5ZFLdk\",label=\"YouTube URL of Meeting\")\n",
    "        submit_url_button = gr.Button(\"Generate Story Angles\")\n",
    "    summary_output = gr.Textbox(label=\"Suggested Story Angles\", interactive=False)\n",
    "\n",
    "    gr.Markdown(\"### Ask Followup Questions\")\n",
    "    gr.ChatInterface(rag_respond)\n",
    "\n",
    "    submit_url_button.click(youtube_summary, inputs=url_input, outputs=summary_output)\n",
    "    gr.Markdown('_Known Issue:_ Sometimes, the chatbot will claim that a topic listed in the story angles did not come up in the meeting, when in fact it did. To get it chatbot to \"remember\" this part of the meeting, simply insist \"yes, that was discussed\". This quirk will hopefully be addressed in future updates.')\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NYCRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
